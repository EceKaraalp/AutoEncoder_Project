{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4b3d2c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-01T19:49:57.232879Z",
     "iopub.status.busy": "2026-01-01T19:49:57.232566Z",
     "iopub.status.idle": "2026-01-01T19:49:57.886790Z",
     "shell.execute_reply": "2026-01-01T19:49:57.886197Z"
    },
    "papermill": {
     "duration": 0.662895,
     "end_time": "2026-01-01T19:49:57.888611",
     "exception": false,
     "start_time": "2026-01-01T19:49:57.225716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f9454",
   "metadata": {
    "papermill": {
     "duration": 0.004538,
     "end_time": "2026-01-01T19:49:57.898080",
     "exception": false,
     "start_time": "2026-01-01T19:49:57.893542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL 1\n",
    "üîΩ Encoder\n",
    "\n",
    "Conv2d + ReLU\n",
    "\n",
    "MaxPool2d (downsampling)\n",
    "\n",
    "Boyut: 128 ‚Üí 64 ‚Üí 32\n",
    "\n",
    "üîº Decoder\n",
    "\n",
    "Upsample (nearest neighbor)\n",
    "\n",
    "Conv2d + ReLU\n",
    "\n",
    "Boyut: 32 ‚Üí 64 ‚Üí 128\n",
    "\n",
    "üéØ √ñƒürenme\n",
    "\n",
    "Supervised Learning\n",
    "\n",
    "Loss: MSELoss\n",
    "\n",
    "Optimizer: Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecff779",
   "metadata": {
    "papermill": {
     "duration": 0.004405,
     "end_time": "2026-01-01T19:49:57.906922",
     "exception": false,
     "start_time": "2026-01-01T19:49:57.902517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1Ô∏è‚É£ GEREKLƒ∞ K√úT√úPHANELER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be9fe19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:49:57.917579Z",
     "iopub.status.busy": "2026-01-01T19:49:57.916628Z",
     "iopub.status.idle": "2026-01-01T19:50:06.776248Z",
     "shell.execute_reply": "2026-01-01T19:50:06.775543Z"
    },
    "papermill": {
     "duration": 8.866719,
     "end_time": "2026-01-01T19:50:06.778071",
     "exception": false,
     "start_time": "2026-01-01T19:49:57.911352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d67861",
   "metadata": {
    "papermill": {
     "duration": 0.004641,
     "end_time": "2026-01-01T19:50:06.787392",
     "exception": false,
     "start_time": "2026-01-01T19:50:06.782751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "2Ô∏è‚É£ SADECE 500 ADET CELEBA G√ñRSEL SE√á"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39da7e29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:50:06.797988Z",
     "iopub.status.busy": "2026-01-01T19:50:06.797558Z",
     "iopub.status.idle": "2026-01-01T19:50:12.434215Z",
     "shell.execute_reply": "2026-01-01T19:50:12.433217Z"
    },
    "papermill": {
     "duration": 5.643839,
     "end_time": "2026-01-01T19:50:12.435752",
     "exception": false,
     "start_time": "2026-01-01T19:50:06.791913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 500 g√∂rsel hazƒ±r\n"
     ]
    }
   ],
   "source": [
    "# CelebA orijinal klas√∂r√º\n",
    "image_dir = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\n",
    "\n",
    "# T√ºm g√∂rseller\n",
    "all_images = os.listdir(image_dir)\n",
    "\n",
    "# Tekrarlanabilirlik\n",
    "random.seed(42)\n",
    "\n",
    "# Sadece 500 g√∂rsel se√ß\n",
    "selected_images = random.sample(all_images, 500)\n",
    "\n",
    "# Hedef klas√∂r\n",
    "target_dir = \"/kaggle/working/celeba_500/images\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Se√ßilen g√∂rselleri kopyala\n",
    "for img_name in selected_images:\n",
    "    shutil.copy(\n",
    "        os.path.join(image_dir, img_name),\n",
    "        os.path.join(target_dir, img_name)\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ 500 g√∂rsel hazƒ±r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81217a23",
   "metadata": {
    "papermill": {
     "duration": 0.004258,
     "end_time": "2026-01-01T19:50:12.444820",
     "exception": false,
     "start_time": "2026-01-01T19:50:12.440562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "3Ô∏è‚É£ DATASET (LR ‚Üí HR PAIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442089b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:50:12.455333Z",
     "iopub.status.busy": "2026-01-01T19:50:12.455065Z",
     "iopub.status.idle": "2026-01-01T19:50:12.460576Z",
     "shell.execute_reply": "2026-01-01T19:50:12.460030Z"
    },
    "papermill": {
     "duration": 0.011934,
     "end_time": "2026-01-01T19:50:12.461834",
     "exception": false,
     "start_time": "2026-01-01T19:50:12.449900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, image_dir, hr_size=128, scale=4):\n",
    "        self.image_dir = image_dir\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "        # HR (ger√ßek y√ºksek √ß√∂z√ºn√ºrl√ºk)\n",
    "        self.hr_transform = transforms.Compose([\n",
    "            transforms.Resize((hr_size, hr_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        # LR (bilerek boz ‚Üí k√º√ß√ºlt ‚Üí tekrar b√ºy√ºt)\n",
    "        self.lr_transform = transforms.Compose([\n",
    "            transforms.Resize((hr_size // scale, hr_size // scale)),\n",
    "            transforms.Resize((hr_size, hr_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(\n",
    "            os.path.join(self.image_dir, self.images[idx])\n",
    "        ).convert(\"RGB\")\n",
    "\n",
    "        hr = self.hr_transform(img)  # hedef\n",
    "        lr = self.lr_transform(img)  # giri≈ü\n",
    "\n",
    "        return lr, hr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc266e4",
   "metadata": {
    "papermill": {
     "duration": 0.004257,
     "end_time": "2026-01-01T19:50:12.470570",
     "exception": false,
     "start_time": "2026-01-01T19:50:12.466313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "4Ô∏è‚É£ DATASET & DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c791cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:50:12.480550Z",
     "iopub.status.busy": "2026-01-01T19:50:12.480254Z",
     "iopub.status.idle": "2026-01-01T19:50:12.488216Z",
     "shell.execute_reply": "2026-01-01T19:50:12.487603Z"
    },
    "papermill": {
     "duration": 0.014894,
     "end_time": "2026-01-01T19:50:12.489667",
     "exception": false,
     "start_time": "2026-01-01T19:50:12.474773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = SuperResolutionDataset(\"/kaggle/working/celeba_500/images\")\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size   = int(0.15 * len(dataset))\n",
    "test_size  = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f0da53",
   "metadata": {
    "papermill": {
     "duration": 0.004348,
     "end_time": "2026-01-01T19:50:12.498613",
     "exception": false,
     "start_time": "2026-01-01T19:50:12.494265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "5Ô∏è‚É£ AUTOENCODER SUPER-RESOLUTION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99dacba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:50:12.508842Z",
     "iopub.status.busy": "2026-01-01T19:50:12.508586Z",
     "iopub.status.idle": "2026-01-01T19:50:12.729625Z",
     "shell.execute_reply": "2026-01-01T19:50:12.728561Z"
    },
    "papermill": {
     "duration": 0.227886,
     "end_time": "2026-01-01T19:50:12.730852",
     "exception": true,
     "start_time": "2026-01-01T19:50:12.502966",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/working/autoencoder_sr.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24/1060908748.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# =========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoencoderSR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/autoencoder_sr.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/autoencoder_sr.pth'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# AUTOENCODER SR ‚Äì POOLING TABANLI (ƒ∞LK MODEL)\n",
    "# Tek g√∂rsel test: LR / SR / HR √ßƒ±ktƒ±larƒ±\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# =========================\n",
    "# DEVICE\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# =========================\n",
    "# MODEL (ƒ∞LK MODEL ‚Äì POOLING)\n",
    "# =========================\n",
    "class AutoencoderSR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------- Encoder --------\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),      # 128 ‚Üí 64\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)       # 64 ‚Üí 32\n",
    "        )\n",
    "\n",
    "        # -------- Decoder --------\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),   # 32 ‚Üí 64\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),   # 64 ‚Üí 128\n",
    "            nn.Conv2d(64, 3, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "# =========================\n",
    "# MODEL Y√úKLE\n",
    "# =========================\n",
    "model = AutoencoderSR().to(device)\n",
    "model.load_state_dict(torch.load(\"/kaggle/working/autoencoder_sr.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# TEST EDƒ∞LECEK RESƒ∞M\n",
    "# (ƒ∞STEDƒ∞ƒûƒ∞N RESMƒ∞ BURAYA KOY)\n",
    "# =========================\n",
    "img_path = \"/kaggle/working/celeba_500/images/003540.jpg\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# =========================\n",
    "# HR ve LR OLU≈ûTUR\n",
    "# =========================\n",
    "hr = T.Resize((128, 128))(img)\n",
    "hr = TF.to_tensor(hr)\n",
    "\n",
    "lr = TF.resize(hr, [32, 32])      # Modelin g√∂rd√ºƒü√º giri≈ü\n",
    "lr_input = lr.unsqueeze(0).to(device)\n",
    "\n",
    "# =========================\n",
    "# SUPER RESOLUTION\n",
    "# =========================\n",
    "with torch.no_grad():\n",
    "    sr = model(lr_input)\n",
    "\n",
    "# =========================\n",
    "# G√ñRSELLE≈ûTƒ∞RME\n",
    "# =========================\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(lr.permute(1,2,0))\n",
    "plt.title(\"Low Resolution (32x32)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(sr.squeeze(0).permute(1,2,0).cpu())\n",
    "plt.title(\"Super Resolved Output\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(hr.permute(1,2,0))\n",
    "plt.title(\"Ground Truth (128x128)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab7c47",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "6Ô∏è‚É£ TRAINING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2278b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:29:52.931170Z",
     "iopub.status.busy": "2026-01-01T18:29:52.930970Z",
     "iopub.status.idle": "2026-01-01T18:29:53.073979Z",
     "shell.execute_reply": "2026-01-01T18:29:53.073199Z",
     "shell.execute_reply.started": "2026-01-01T18:29:52.931150Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoencoderSR().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cebd8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "7Ô∏è‚É£ TRAIN + VALIDATION LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63544320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:29:53.075361Z",
     "iopub.status.busy": "2026-01-01T18:29:53.075123Z",
     "iopub.status.idle": "2026-01-01T18:30:27.172779Z",
     "shell.execute_reply": "2026-01-01T18:30:27.171973Z",
     "shell.execute_reply.started": "2026-01-01T18:29:53.075344Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for lr, hr in train_loader:\n",
    "        lr, hr = lr.to(device), hr.to(device)\n",
    "\n",
    "        out = model(lr)\n",
    "        loss = criterion(out, hr)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # ---- VALIDATION ----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for lr, hr in val_loader:\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            val_loss += criterion(model(lr), hr).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf240d6a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "8Ô∏è‚É£ LOSS GRAFƒ∞ƒûƒ∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3a677",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:30:27.173756Z",
     "iopub.status.busy": "2026-01-01T18:30:27.173552Z",
     "iopub.status.idle": "2026-01-01T18:30:27.368903Z",
     "shell.execute_reply": "2026-01-01T18:30:27.368055Z",
     "shell.execute_reply.started": "2026-01-01T18:30:27.173735Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"Train\")\n",
    "plt.plot(val_losses, label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Progress\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3110651d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "9Ô∏è‚É£ MODELƒ∞ KAYDET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedfcd73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:30:27.369992Z",
     "iopub.status.busy": "2026-01-01T18:30:27.369775Z",
     "iopub.status.idle": "2026-01-01T18:30:27.379111Z",
     "shell.execute_reply": "2026-01-01T18:30:27.378288Z",
     "shell.execute_reply.started": "2026-01-01T18:30:27.369972Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/kaggle/working/autoencoder_sr.pth\")\n",
    "print(\"‚úÖ Model kaydedildi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3591ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:48:02.923586Z",
     "iopub.status.busy": "2026-01-01T18:48:02.922904Z",
     "iopub.status.idle": "2026-01-01T18:48:02.955788Z",
     "shell.execute_reply": "2026-01-01T18:48:02.954851Z",
     "shell.execute_reply.started": "2026-01-01T18:48:02.923562Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AUTOENCODER SUPER RESOLUTION - POOLING BASED (FIRST MODEL)\n",
    "# Encoder: MaxPooling\n",
    "# Decoder: Upsample\n",
    "# Aray√ºz YOK ‚Äì Manuel resim test\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# =========================\n",
    "# DEVICE\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# =========================\n",
    "# MODEL TANIMI (POOLING)\n",
    "# =========================\n",
    "class AutoencoderSR_Pooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------- ENCODER --------\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),          # 32 ‚Üí 16\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)           # 16 ‚Üí 8\n",
    "        )\n",
    "\n",
    "        # -------- DECODER --------\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),  # 8 ‚Üí 16\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),  # 16 ‚Üí 32\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),  # 32 ‚Üí 64\n",
    "            nn.Conv2d(64, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),  # 64 ‚Üí 128\n",
    "            nn.Conv2d(32, 3, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "# =========================\n",
    "# MODEL Y√úKLE\n",
    "# =========================\n",
    "model_path = \"/kaggle/working/autoencoder_sr.pth\"  # ‚Üê pooling modelin\n",
    "model = AutoencoderSR_Pooling().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# TEST EDƒ∞LECEK RESƒ∞M\n",
    "# =========================\n",
    "test_img_path = \"/kaggle/working/celeba_500/images/003540.jpg\"  # ‚Üê istediƒüin resim\n",
    "img = Image.open(test_img_path).convert(\"RGB\")\n",
    "\n",
    "# =========================\n",
    "# HR / LR OLU≈ûTUR\n",
    "# =========================\n",
    "hr = TF.to_tensor(T.Resize((128, 128))(img))\n",
    "lr = TF.resize(hr, [32, 32])\n",
    "\n",
    "lr_input = lr.unsqueeze(0).to(device)\n",
    "\n",
    "# =========================\n",
    "# SUPER RESOLUTION\n",
    "# =========================\n",
    "with torch.no_grad():\n",
    "    sr = model(lr_input)\n",
    "\n",
    "# =========================\n",
    "# G√ñRSELLE≈ûTƒ∞RME\n",
    "# =========================\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(lr.permute(1,2,0))\n",
    "plt.title(\"Low Resolution (32x32)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(sr.squeeze(0).permute(1,2,0).cpu())\n",
    "plt.title(\"Super Resolved (Pooling Model)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(hr.permute(1,2,0))\n",
    "plt.title(\"Original / Ground Truth (128x128)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c081d863",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "üîü GRADIO ARAY√úZ√ú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19808a77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:30:27.380398Z",
     "iopub.status.busy": "2026-01-01T18:30:27.380096Z",
     "iopub.status.idle": "2026-01-01T18:30:30.810455Z",
     "shell.execute_reply": "2026-01-01T18:30:30.809713Z",
     "shell.execute_reply.started": "2026-01-01T18:30:27.380369Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc8e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:30:30.811717Z",
     "iopub.status.busy": "2026-01-01T18:30:30.811451Z",
     "iopub.status.idle": "2026-01-01T18:30:35.478392Z",
     "shell.execute_reply": "2026-01-01T18:30:35.477756Z",
     "shell.execute_reply.started": "2026-01-01T18:30:30.811691Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import gradio as gr\n",
    "\n",
    "# =========================\n",
    "# DEVICE\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# =========================\n",
    "# MODELƒ∞ Y√úKLE\n",
    "# =========================\n",
    "model = AutoencoderSR().to(device)\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\"/kaggle/working/autoencoder_sr.pth\", map_location=device)\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# SUPER RESOLUTION FONKSƒ∞YONU\n",
    "# =========================\n",
    "def super_resolve(image):\n",
    "    # PIL ‚Üí Tensor (HR)\n",
    "    hr = TF.to_tensor(image)\n",
    "\n",
    "    # Bilerek d√º≈ü√ºr (LR sim√ºlasyonu)\n",
    "    lr = TF.resize(hr, [32, 32])\n",
    "\n",
    "    # Modele uygun hale getir\n",
    "    lr_up = TF.resize(lr, [128, 128]).unsqueeze(0).to(device)\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        sr = model(lr_up)\n",
    "\n",
    "    # G√ñRSELLE≈ûTƒ∞RME ƒ∞√áƒ∞N B√úY√úT\n",
    "    lr_vis = TF.resize(lr, [256, 256])\n",
    "    sr_vis = TF.resize(sr.squeeze(0), [256, 256])\n",
    "\n",
    "    return (\n",
    "        lr_vis.permute(1,2,0).cpu().numpy(),\n",
    "        sr_vis.permute(1,2,0).cpu().numpy()\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# GRADIO ARAY√úZ\n",
    "import gradio as gr\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=super_resolve,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Input Image\"),\n",
    "    outputs=[\n",
    "        gr.Image(label=\"Low Resolution (64x64, Blurred)\"),\n",
    "        gr.Image(label=\"Super-Resolved Output (128x128)\")\n",
    "    ],\n",
    "    title=\"Advanced Autoencoder-based Image Super-Resolution\",\n",
    "    description=\"Low-resolution images are generated using downsampling and Gaussian blur, consistent with the training procedure.\"\n",
    ")\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d571fe8a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# MODEL 2\n",
    "Pooling yerine stride kullanƒ±ldƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d47e70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:30:35.479868Z",
     "iopub.status.busy": "2026-01-01T18:30:35.479328Z",
     "iopub.status.idle": "2026-01-01T18:31:01.423297Z",
     "shell.execute_reply": "2026-01-01T18:31:01.422572Z",
     "shell.execute_reply.started": "2026-01-01T18:30:35.479846Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AUTOENCODER SUPER RESOLUTION (STRIDE-BASED DOWNSAMPLING)\n",
    "# Pooling YOK ‚Äì Stride kullanƒ±ldƒ±\n",
    "# Dataset: CelebA (sadece 500 g√∂r√ºnt√º)\n",
    "# ============================================================\n",
    "\n",
    "# =========================\n",
    "# 1. IMPORTLAR\n",
    "# =========================\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# =========================\n",
    "# 2. DEVICE AYARI\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ============================================================\n",
    "# 3. DATASET HAZIRLAMA (SADECE 500 G√ñR√úNT√ú)\n",
    "# ============================================================\n",
    "\n",
    "image_dir = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\n",
    "all_images = os.listdir(image_dir)\n",
    "\n",
    "random.seed(42)\n",
    "selected_images = random.sample(all_images, 500)\n",
    "\n",
    "target_dir = \"/kaggle/working/celeba_500/images\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "for img_name in selected_images:\n",
    "    shutil.copy(\n",
    "        os.path.join(image_dir, img_name),\n",
    "        os.path.join(target_dir, img_name)\n",
    "    )\n",
    "\n",
    "print(\"Toplam se√ßilen g√∂r√ºnt√º:\", len(selected_images))\n",
    "\n",
    "# ============================================================\n",
    "# 4. DATASET SINIFI (LR ‚Üí HR)\n",
    "# ============================================================\n",
    "\n",
    "class CelebASRDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "        self.hr_transform = T.Compose([\n",
    "            T.Resize((128, 128)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        hr = self.hr_transform(img)\n",
    "        lr = TF.resize(hr, [32, 32])   # Low Resolution\n",
    "\n",
    "        return lr, hr\n",
    "\n",
    "dataset = CelebASRDataset(target_dir)\n",
    "\n",
    "# ============================================================\n",
    "# 5. TRAIN / VAL / TEST SPLIT\n",
    "# ============================================================\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size   = int(0.15 * len(dataset))\n",
    "test_size  = len(dataset) - train_size - val_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "# ============================================================\n",
    "# 6. MODEL ‚Äì STRIDE-BASED AUTOENCODER\n",
    "# Pooling YOK ‚ùå\n",
    "# Downsampling = stride=2\n",
    "# ============================================================\n",
    "\n",
    "class AutoencoderSR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # =====================\n",
    "        # ENCODER (LR ‚Üí LATENT)\n",
    "        # =====================\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=2, padding=1),   # 32 ‚Üí 16\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), # 16 ‚Üí 8\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # =====================\n",
    "        # DECODER (LATENT ‚Üí HR)\n",
    "        # =====================\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),               # 8 ‚Üí 16\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),               # 16 ‚Üí 32\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),               # 32 ‚Üí 64\n",
    "            nn.Conv2d(64, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),               # 64 ‚Üí 128\n",
    "            nn.Conv2d(32, 3, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = AutoencoderSR().to(device)\n",
    "\n",
    "# ============================================================\n",
    "# 7. LOSS & OPTIMIZER\n",
    "# ============================================================\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ============================================================\n",
    "# 8. TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "num_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # -------- TRAIN --------\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for lr_imgs, hr_imgs in train_loader:\n",
    "        lr_imgs = lr_imgs.to(device)\n",
    "        hr_imgs = hr_imgs.to(device)\n",
    "\n",
    "        outputs = model(lr_imgs)\n",
    "        loss = criterion(outputs, hr_imgs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # -------- VALIDATION --------\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in val_loader:\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            hr_imgs = hr_imgs.to(device)\n",
    "\n",
    "            outputs = model(lr_imgs)\n",
    "            loss = criterion(outputs, hr_imgs)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9. MODEL KAYDETME\n",
    "# ============================================================\n",
    "\n",
    "model_path = \"/kaggle/working/autoencoder_stride_sr.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Model kaydedildi:\", model_path)\n",
    "\n",
    "# ============================================================\n",
    "# 10. LOSS GRAFƒ∞ƒûƒ∞\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 11. GRADIO ARAY√úZ√ú\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d30707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T18:47:03.359810Z",
     "iopub.status.busy": "2026-01-01T18:47:03.359111Z",
     "iopub.status.idle": "2026-01-01T18:47:03.710818Z",
     "shell.execute_reply": "2026-01-01T18:47:03.710042Z",
     "shell.execute_reply.started": "2026-01-01T18:47:03.359784Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AUTOENCODER SUPER RESOLUTION - SINGLE IMAGE TEST\n",
    "# Model: Stride-based Autoencoder (32x32 ‚Üí 128x128)\n",
    "# Aray√ºz YOK ‚Äì Manuel resim test\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# =========================\n",
    "# DEVICE\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# =========================\n",
    "# MODEL TANIMI\n",
    "# =========================\n",
    "class AutoencoderSR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=2, padding=1),   # 32 ‚Üí 16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), # 16 ‚Üí 8\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),  # 8 ‚Üí 16\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),  # 16 ‚Üí 32\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),  # 32 ‚Üí 64\n",
    "            nn.Conv2d(64, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),  # 64 ‚Üí 128\n",
    "            nn.Conv2d(32, 3, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "# =========================\n",
    "# MODEL Y√úKLE\n",
    "# =========================\n",
    "model_path = \"/kaggle/working/autoencoder_stride_sr.pth\"  # ‚Üê gerekirse deƒüi≈ütir\n",
    "model = AutoencoderSR().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# TEST EDƒ∞LECEK RESƒ∞M\n",
    "# =========================\n",
    "test_img_path = \"/kaggle/working/celeba_500/images/003540.jpg\"  # ‚Üê KENDƒ∞ RESMƒ∞N\n",
    "\n",
    "img = Image.open(test_img_path).convert(\"RGB\")\n",
    "\n",
    "# =========================\n",
    "# HR / LR OLU≈ûTUR\n",
    "# =========================\n",
    "hr = TF.to_tensor(T.Resize((128, 128))(img))\n",
    "lr = TF.resize(hr, [32, 32])\n",
    "\n",
    "lr_input = lr.unsqueeze(0).to(device)\n",
    "\n",
    "# =========================\n",
    "# SUPER RESOLUTION\n",
    "# =========================\n",
    "with torch.no_grad():\n",
    "    sr = model(lr_input)\n",
    "\n",
    "# =========================\n",
    "# G√ñRSELLE≈ûTƒ∞RME\n",
    "# =========================\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(lr.permute(1,2,0))\n",
    "plt.title(\"Low Resolution (32x32)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(sr.squeeze(0).permute(1,2,0).cpu())\n",
    "plt.title(\"Super Resolved (Model Output)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(hr.permute(1,2,0))\n",
    "plt.title(\"Original / Ground Truth (128x128)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142507d3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# MODEL 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b497762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:03:36.528366Z",
     "iopub.status.busy": "2026-01-01T19:03:36.527678Z",
     "iopub.status.idle": "2026-01-01T19:04:14.250955Z",
     "shell.execute_reply": "2026-01-01T19:04:14.250183Z",
     "shell.execute_reply.started": "2026-01-01T19:03:36.528341Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# IMPORTS & DEVICE\n",
    "# =========================\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# =========================\n",
    "# DATASET (REALISTIC LR)\n",
    "# =========================\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, image_dir, hr_size=128):\n",
    "        self.image_dir = image_dir\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "        # High-resolution\n",
    "        self.hr_transform = transforms.Compose([\n",
    "            transforms.Resize((hr_size, hr_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        # Low-resolution (downsample + blur)\n",
    "        self.lr_transform = transforms.Compose([\n",
    "            transforms.Resize((hr_size // 2, hr_size // 2)),\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        hr = self.hr_transform(img)   # 128x128\n",
    "        lr = self.lr_transform(img)   # 64x64\n",
    "\n",
    "        return lr, hr\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class AutoencoderSR_Advanced(nn.Module):\n",
    "    def __init__(self, num_blocks=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            *[ResidualBlock(64) for _ in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Conv2d(64, 3, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.head(x)\n",
    "        body_out = self.body(feat)\n",
    "        up = self.upsample(body_out)\n",
    "        out = self.tail(up)\n",
    "\n",
    "        # üîπ Global Skip Connection\n",
    "        skip = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "        return out + skip\n",
    "\n",
    "# =========================\n",
    "# DATA PREPARATION\n",
    "# =========================\n",
    "image_dir = \"/kaggle/working/celeba_500/images\"\n",
    "dataset = SuperResolutionDataset(image_dir)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size   = int(0.15 * dataset_size)\n",
    "test_size  = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING\n",
    "# =========================\n",
    "model = AutoencoderSR_Advanced().to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Training started\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for lr_imgs, hr_imgs in train_loader:\n",
    "        lr_imgs = lr_imgs.to(device)\n",
    "        hr_imgs = hr_imgs.to(device)\n",
    "\n",
    "        outputs = model(lr_imgs)\n",
    "        loss = criterion(outputs, hr_imgs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # ---- VALIDATION ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in val_loader:\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            hr_imgs = hr_imgs.to(device)\n",
    "\n",
    "            outputs = model(lr_imgs)\n",
    "            loss = criterion(outputs, hr_imgs)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# SAVE MODEL\n",
    "# =========================\n",
    "torch.save(model.state_dict(), \"autoencoder_sr_advanced.pth\")\n",
    "print(\"Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb4532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:10:12.712482Z",
     "iopub.status.busy": "2026-01-01T19:10:12.712203Z",
     "iopub.status.idle": "2026-01-01T19:10:13.039983Z",
     "shell.execute_reply": "2026-01-01T19:10:13.039197Z",
     "shell.execute_reply.started": "2026-01-01T19:10:12.712463Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AUTOENCODER SUPER RESOLUTION - SINGLE IMAGE TEST\n",
    "# Model: Stride-based Autoencoder (32x32 ‚Üí 128x128)\n",
    "# Aray√ºz YOK ‚Äì Manuel resim test\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# =========================\n",
    "# DEVICE\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Doƒüru model sƒ±nƒ±fƒ±\n",
    "\n",
    "# =========================\n",
    "# MODEL TANIMI\n",
    "# =========================\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# =========================\n",
    "# MODEL Y√úKLE\n",
    "# =========================\n",
    "# Doƒüru model sƒ±nƒ±fƒ±\n",
    "model = AutoencoderSR_Advanced().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# TEST EDƒ∞LECEK RESƒ∞M\n",
    "# =========================\n",
    "test_img_path = \"/kaggle/working/celeba_500/images/003540.jpg\"  # ‚Üê Kendi se√ßtiƒüin resim\n",
    "img = Image.open(test_img_path).convert(\"RGB\")\n",
    "\n",
    "# =========================\n",
    "# HR / LR OLU≈ûTUR\n",
    "# =========================\n",
    "hr = TF.to_tensor(T.Resize((128, 128))(img))   # Ground truth\n",
    "lr = TF.resize(hr, [32, 32])                   # Low-resolution\n",
    "\n",
    "lr_input = lr.unsqueeze(0).to(device)          # Batch dimension ekle\n",
    "\n",
    "# =========================\n",
    "# SUPER RESOLUTION\n",
    "# =========================\n",
    "with torch.no_grad():\n",
    "    sr = model(lr_input)\n",
    "\n",
    "# =========================\n",
    "# G√ñRSELLE≈ûTƒ∞RME\n",
    "# =========================\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "# Low Resolution\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(lr.permute(1,2,0).cpu())\n",
    "plt.title(\"Low Resolution (32x32)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Super Resolved\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(sr.squeeze(0).permute(1,2,0).cpu())\n",
    "plt.title(\"Super Resolved (Model Output)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# High Resolution / Ground Truth\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(hr.permute(1,2,0).cpu())\n",
    "plt.title(\"Original / Ground Truth (128x128)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a3519d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# MODEL 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d855629",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:29:50.401639Z",
     "iopub.status.busy": "2026-01-01T19:29:50.401100Z",
     "iopub.status.idle": "2026-01-01T19:31:14.023190Z",
     "shell.execute_reply": "2026-01-01T19:31:14.022067Z",
     "shell.execute_reply.started": "2026-01-01T19:29:50.401617Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AUTOENCODER SUPER RESOLUTION - FULL BLOCK (TRAIN + SINGLE IMAGE TEST)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# DEVICE\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, image_dir, hr_size=128):\n",
    "        self.image_dir = image_dir\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "        # HR Transform\n",
    "        self.hr_transform = transforms.Compose([\n",
    "            transforms.Resize((hr_size, hr_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        # üî• Daha ger√ßek√ßi LR Transform\n",
    "        self.lr_transform = transforms.Compose([\n",
    "            transforms.Resize((hr_size // 2, hr_size // 2)),\n",
    "            transforms.GaussianBlur(kernel_size=3),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(os.path.join(self.image_dir, self.images[idx])).convert(\"RGB\")\n",
    "        hr = self.hr_transform(img)\n",
    "        lr = self.lr_transform(img)\n",
    "        return lr, hr\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class AutoencoderSR_Advanced(nn.Module):\n",
    "    def __init__(self, num_blocks=8):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.body = nn.Sequential(\n",
    "            *[ResidualBlock(64) for _ in range(num_blocks)]\n",
    "        )\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Conv2d(64, 3, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        x = self.body(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.tail(x)\n",
    "        return x\n",
    "\n",
    "# =========================\n",
    "# EDGE LOSS\n",
    "# =========================\n",
    "def edge_loss(sr, hr):\n",
    "    sobel_x = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]],\n",
    "                            dtype=torch.float32).view(1,1,3,3).to(sr.device)\n",
    "    sobel_y = torch.tensor([[1,2,1],[0,0,0],[-1,-2,-1]],\n",
    "                            dtype=torch.float32).view(1,1,3,3).to(sr.device)\n",
    "\n",
    "    def sobel(img):\n",
    "        gray = img.mean(dim=1, keepdim=True)\n",
    "        gx = F.conv2d(gray, sobel_x, padding=1)\n",
    "        gy = F.conv2d(gray, sobel_y, padding=1)\n",
    "        return torch.sqrt(gx**2 + gy**2)\n",
    "\n",
    "    return F.l1_loss(sobel(sr), sobel(hr))\n",
    "\n",
    "# =========================\n",
    "# DATA PREPARATION\n",
    "# =========================\n",
    "image_dir = \"/kaggle/working/celeba_500/images\"\n",
    "dataset = SuperResolutionDataset(image_dir)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size   = int(0.15 * len(dataset))\n",
    "test_size  = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "# =========================\n",
    "# MODEL / OPTIMIZER / LOSS\n",
    "# =========================\n",
    "model = AutoencoderSR_Advanced().to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 50\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "print(\"üöÄ Training Started\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for lr_imgs, hr_imgs in train_loader:\n",
    "        lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
    "        sr = model(lr_imgs)\n",
    "        loss = criterion(sr, hr_imgs) + 0.1 * edge_loss(sr, hr_imgs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in val_loader:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
    "            sr = model(lr_imgs)\n",
    "            loss = criterion(sr, hr_imgs)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# LOSS GRAFƒ∞ƒûƒ∞\n",
    "# =========================\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# =========================\n",
    "# MODEL KAYDET\n",
    "# =========================\n",
    "torch.save(model.state_dict(), \"sr_autoencoder_final.pth\")\n",
    "print(\"‚úÖ Model saved.\")\n",
    "\n",
    "# =========================\n",
    "# TEK RESƒ∞M TESTƒ∞\n",
    "# =========================\n",
    "test_img_path = \"/kaggle/working/celeba_500/images/003540.jpg\"\n",
    "img = Image.open(test_img_path).convert(\"RGB\")\n",
    "\n",
    "hr = TF.to_tensor(transforms.Resize((128,128))(img))\n",
    "lr = TF.resize(hr, [64,64])\n",
    "\n",
    "lr_input = lr.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sr = model(lr_input)\n",
    "\n",
    "# =========================\n",
    "# G√ñRSELLE≈ûTƒ∞RME\n",
    "# =========================\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(lr.permute(1,2,0).cpu())\n",
    "plt.title(\"Low Resolution (64x64)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(sr.squeeze(0).permute(1,2,0).cpu())\n",
    "plt.title(\"Super Resolved\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(hr.permute(1,2,0).cpu())\n",
    "plt.title(\"Original / HR (128x128)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1493a2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 29561,
     "sourceId": 37705,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31240,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.683109,
   "end_time": "2026-01-01T19:50:14.457398",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-01T19:49:54.774289",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
